{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format, SFTConfig\n",
    "from dotenv import load_dotenv\n",
    "import bitsandbytes as bnb\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface and WandB authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "HF_KEY = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"C:/Users/User/Data Science/Deep Learning/Generative AI/Fine Tuning LLMs/fine-tuning llama 3.2 1B/research.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0551969e89fa4b85b33759cd9c04c529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Data Science\\Deep Learning\\Generative AI\\Fine Tuning LLMs\\fine-tuning llama 3.2 1B\\wandb\\run-20241004_204804-7pibddmb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/victordareai/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset/runs/7pibddmb' target=\"_blank\">youthful-surf-4</a></strong> to <a href='https://wandb.ai/victordareai/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/victordareai/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset' target=\"_blank\">https://wandb.ai/victordareai/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/victordareai/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset/runs/7pibddmb' target=\"_blank\">https://wandb.ai/victordareai/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset/runs/7pibddmb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='Fine-tune Llama 3.2 on Customer Support Dataset',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "new_model = \"llama-3.2-1b-it-Ecommerce-ChatBot\"\n",
    "dataset_name = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the data type and attention implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try lower version of bitsandbytes and use the load_in_4bit=True directly on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\utils\\modeling.py:856: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26872"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIrst, we will train with only 1000 samples of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.shuffle(seed=65).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset.shuffle(seed=65).select(range(1000, 1200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['instruction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing to match our chat template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change name  to 'Procurtech Assistant' when trying out the next training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    If the question is out of context and not related to your job as a customer service agent, let the customer know that you can not help and they should look elsewhere for answers.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    base_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model, trust_remote_code=True)\n",
    "\n",
    "    instruction = \"\"\"You are a top-rated customer service agent named 'Procurtech Assistant'. \n",
    "        Be polite to customers and answer all their questions.\n",
    "        If the question is out of context and not related to your job as a customer service agent, let the customer know that you can not help and they should look elsewhere for answers.\n",
    "        \"\"\"\n",
    "\n",
    "    row_json = [{\"role\": \"system\", \"content\": instruction},\n",
    "                {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "                {\"role\": \"assistant\", \"content\": row[\"response\"]}]\n",
    "\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d037cc454a84181a8fd5983ccf783a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(format_chat_template, num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8537a14e69ad4660bcb8b89de01cadf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = test_dataset.map(format_chat_template, num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f023342963248ba96c049389f99e620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01eb9cddd3094ffdb7ec9e8f9307c70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Victorano/customer-support-1k/commit/157c1da2fb0e842461161ea69d8551fc26715b4d', commit_message='Upload dataset', commit_description='', oid='157c1da2fb0e842461161ea69d8551fc26715b4d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"customer-support-1k\", token=HF_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39fcb305d2545249bf6e44a456db776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdc9c5463f0482286968261d7a597c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Victorano/customer-support-test-200/commit/af6cd0525f81ae93586b3035fb3e5adb819a651b', commit_message='Upload dataset', commit_description='', oid='af6cd0525f81ae93586b3035fb3e5adb819a651b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.push_to_hub(\"customer-support-test-200\", token=HF_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a top-rated customer service agent named John. \\n        Be polite to customers and answer all their questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nwhere do i enter a different shipping address<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAh, I understand that you\\'re looking to enter a different shipping address. Allow me to guide you through the process:\\n\\n1. Log in to your account on our website.\\n2. Navigate to the \"My Account\" or \"Profile\" section, which can usually be found in the top right corner of the page.\\n3. Look for the \"Shipping Addresses\" or similar option. It may also be labeled as \"Manage Addresses\" or \"Delivery Information.\"\\n4. Click on that option to access your saved addresses.\\n5. To enter a different shipping address, you\\'ll most likely have the choice to either \"Edit\" an existing address or \"Add a New Address.\"\\n6. If you choose to edit, find the address you want to update and click on the \"Edit\" or \"Modify\" button. Make your changes and then save.\\n7. If you want to add a completely new address, click on the \"Add a New Address\" or a similar option. Fill in the necessary details and save.\\n\\nRemember to double-check all the information you enter to ensure the accuracy of your shipping address. If you encounter any issues or have further questions, please don\\'t hesitate to ask. We\\'re here to support you every step of the way!<|eot_id|>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the saved datasets from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Victorano/customer-support-1k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a top-rated customer service agent named John. \\n        Be polite to customers and answer all their questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nchange to {{Account Type}} acount<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThank you for expressing interest in upgrading to the {{Account Type}} account! I\\'d be delighted to guide you through the process:\\n\\n1. Access Your Account: Start by logging in to our platform using your credentials.\\n2. Go to Account Settings: Once you\\'re logged in, navigate to the \"Account Settings\" section.\\n3. Upgrade Option: Look for the option that allows you to upgrade your account type.\\n4. Select {{Account Type}} Account: Choose the \"{{Account Type}} Account\" option from the available choices.\\n5. Follow the Prompts: Follow the prompts on the screen to complete the upgrade process successfully.\\n\\nShould you encounter any difficulties during the upgrade or have any additional questions, please don\\'t hesitate to reach out. We value your decision to upgrade to the {{Account Type}} account and will ensure a seamless transition. Enjoy the exclusive benefits that come with the {{Account Type}} account!<|eot_id|>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"Victorano/customer-support-test-200\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a top-rated customer service agent named John. \\n        Be polite to customers and answer all their questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nis it possible to buy several items<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThank you for your interest in purchasing several items from us! We have a wide range of products available for you to choose from. To place your order, simply browse through our online catalog and add the items you want to your shopping cart. Once you've selected all the items you wish to purchase, proceed to the checkout page and follow the prompts to complete your order. If you have any questions or need assistance during the process, feel free to reach out to our customer service team who will be more than happy to help. Happy shopping!<|eot_id|>\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 200)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Model\n",
    "extract the linear model name from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down_proj', 'o_proj', 'up_proj', 'q_proj', 'v_proj', 'gate_proj', 'k_proj']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Linear Module name to create the LoRA Adapter, this is only what we will be fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lora Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128256"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = SFTConfig(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    # report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c496316a63654b79a0fab66cf277b5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaaa3a9d4da14bdca34007794d3fb4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e695ea74124801b0636bfdda29ed27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888889738, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Data Science\\Deep Learning\\Generative AI\\Fine Tuning LLMs\\fine-tuning llama 3.2 1B\\wandb\\run-20241006_025004-ctpm5nyj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/victordareai/huggingface/runs/ctpm5nyj' target=\"_blank\">llama-3.2-3b-it-Ecommerce-ChatBot</a></strong> to <a href='https://wandb.ai/victordareai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/victordareai/huggingface' target=\"_blank\">https://wandb.ai/victordareai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/victordareai/huggingface/runs/ctpm5nyj' target=\"_blank\">https://wandb.ai/victordareai/huggingface/runs/ctpm5nyj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923c6755dbcf4fea82e7fbed2d166f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.082, 'grad_norm': 2.9098920822143555, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 2.1738, 'grad_norm': 3.2849280834198, 'learning_rate': 4e-05, 'epoch': 0.0}\n",
      "{'loss': 2.3609, 'grad_norm': 3.45859432220459, 'learning_rate': 6e-05, 'epoch': 0.01}\n",
      "{'loss': 2.4198, 'grad_norm': 2.80244517326355, 'learning_rate': 8e-05, 'epoch': 0.01}\n",
      "{'loss': 2.2475, 'grad_norm': 2.3314037322998047, 'learning_rate': 0.0001, 'epoch': 0.01}\n",
      "{'loss': 2.1225, 'grad_norm': 1.9117026329040527, 'learning_rate': 0.00012, 'epoch': 0.01}\n",
      "{'loss': 1.8666, 'grad_norm': 1.7173044681549072, 'learning_rate': 0.00014, 'epoch': 0.01}\n",
      "{'loss': 1.8698, 'grad_norm': 1.875807285308838, 'learning_rate': 0.00016, 'epoch': 0.02}\n",
      "{'loss': 1.6927, 'grad_norm': 1.9178657531738281, 'learning_rate': 0.00018, 'epoch': 0.02}\n",
      "{'loss': 1.6488, 'grad_norm': 1.7952669858932495, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.2951, 'grad_norm': 1.8028624057769775, 'learning_rate': 0.0001995918367346939, 'epoch': 0.02}\n",
      "{'loss': 1.3298, 'grad_norm': 2.0094237327575684, 'learning_rate': 0.00019918367346938775, 'epoch': 0.02}\n",
      "{'loss': 1.042, 'grad_norm': 1.8057894706726074, 'learning_rate': 0.00019877551020408164, 'epoch': 0.03}\n",
      "{'loss': 1.0187, 'grad_norm': 1.6767688989639282, 'learning_rate': 0.00019836734693877553, 'epoch': 0.03}\n",
      "{'loss': 1.019, 'grad_norm': 1.796892762184143, 'learning_rate': 0.00019795918367346938, 'epoch': 0.03}\n",
      "{'loss': 1.0239, 'grad_norm': 1.712019920349121, 'learning_rate': 0.00019755102040816327, 'epoch': 0.03}\n",
      "{'loss': 0.9497, 'grad_norm': 1.6047184467315674, 'learning_rate': 0.00019714285714285716, 'epoch': 0.03}\n",
      "{'loss': 0.7867, 'grad_norm': 1.4462982416152954, 'learning_rate': 0.00019673469387755104, 'epoch': 0.04}\n",
      "{'loss': 0.8495, 'grad_norm': 1.5172209739685059, 'learning_rate': 0.0001963265306122449, 'epoch': 0.04}\n",
      "{'loss': 0.965, 'grad_norm': 1.5170869827270508, 'learning_rate': 0.0001959183673469388, 'epoch': 0.04}\n",
      "{'loss': 0.953, 'grad_norm': 1.96201491355896, 'learning_rate': 0.00019551020408163265, 'epoch': 0.04}\n",
      "{'loss': 1.0427, 'grad_norm': 1.8717280626296997, 'learning_rate': 0.00019510204081632656, 'epoch': 0.04}\n",
      "{'loss': 0.8454, 'grad_norm': 2.808203935623169, 'learning_rate': 0.00019469387755102042, 'epoch': 0.05}\n",
      "{'loss': 0.9524, 'grad_norm': 3.8821120262145996, 'learning_rate': 0.0001942857142857143, 'epoch': 0.05}\n",
      "{'loss': 0.968, 'grad_norm': 1.5417183637619019, 'learning_rate': 0.00019387755102040816, 'epoch': 0.05}\n",
      "{'loss': 0.8379, 'grad_norm': 1.4869568347930908, 'learning_rate': 0.00019346938775510205, 'epoch': 0.05}\n",
      "{'loss': 1.0334, 'grad_norm': 1.388441801071167, 'learning_rate': 0.00019306122448979593, 'epoch': 0.05}\n",
      "{'loss': 0.9823, 'grad_norm': 1.3755313158035278, 'learning_rate': 0.0001926530612244898, 'epoch': 0.06}\n",
      "{'loss': 1.0124, 'grad_norm': 1.3478128910064697, 'learning_rate': 0.00019224489795918368, 'epoch': 0.06}\n",
      "{'loss': 0.7407, 'grad_norm': 1.353702425956726, 'learning_rate': 0.00019183673469387756, 'epoch': 0.06}\n",
      "{'loss': 0.851, 'grad_norm': 1.5185576677322388, 'learning_rate': 0.00019142857142857145, 'epoch': 0.06}\n",
      "{'loss': 1.217, 'grad_norm': 1.8464739322662354, 'learning_rate': 0.0001910204081632653, 'epoch': 0.06}\n",
      "{'loss': 0.8258, 'grad_norm': 1.2955125570297241, 'learning_rate': 0.0001906122448979592, 'epoch': 0.07}\n",
      "{'loss': 0.9461, 'grad_norm': 1.327008605003357, 'learning_rate': 0.00019020408163265305, 'epoch': 0.07}\n",
      "{'loss': 0.9068, 'grad_norm': 1.4303659200668335, 'learning_rate': 0.00018979591836734697, 'epoch': 0.07}\n",
      "{'loss': 0.7795, 'grad_norm': 1.0589439868927002, 'learning_rate': 0.00018938775510204083, 'epoch': 0.07}\n",
      "{'loss': 0.7904, 'grad_norm': 1.2329071760177612, 'learning_rate': 0.0001889795918367347, 'epoch': 0.07}\n",
      "{'loss': 0.8711, 'grad_norm': 1.2903881072998047, 'learning_rate': 0.00018857142857142857, 'epoch': 0.08}\n",
      "{'loss': 0.8117, 'grad_norm': 1.2454392910003662, 'learning_rate': 0.00018816326530612246, 'epoch': 0.08}\n",
      "{'loss': 0.7488, 'grad_norm': 1.1285167932510376, 'learning_rate': 0.00018775510204081634, 'epoch': 0.08}\n",
      "{'loss': 0.7974, 'grad_norm': 1.3030979633331299, 'learning_rate': 0.00018734693877551023, 'epoch': 0.08}\n",
      "{'loss': 0.852, 'grad_norm': 1.2861266136169434, 'learning_rate': 0.0001869387755102041, 'epoch': 0.08}\n",
      "{'loss': 0.8321, 'grad_norm': 1.364457368850708, 'learning_rate': 0.00018653061224489797, 'epoch': 0.09}\n",
      "{'loss': 0.7905, 'grad_norm': 1.5875014066696167, 'learning_rate': 0.00018612244897959183, 'epoch': 0.09}\n",
      "{'loss': 0.6691, 'grad_norm': 1.5160083770751953, 'learning_rate': 0.00018571428571428572, 'epoch': 0.09}\n",
      "{'loss': 0.8618, 'grad_norm': 1.468906283378601, 'learning_rate': 0.0001853061224489796, 'epoch': 0.09}\n",
      "{'loss': 0.749, 'grad_norm': 1.3393250703811646, 'learning_rate': 0.00018489795918367346, 'epoch': 0.09}\n",
      "{'loss': 0.8707, 'grad_norm': 1.6051613092422485, 'learning_rate': 0.00018448979591836735, 'epoch': 0.1}\n",
      "{'loss': 0.8143, 'grad_norm': 1.7122396230697632, 'learning_rate': 0.00018408163265306123, 'epoch': 0.1}\n",
      "{'loss': 0.779, 'grad_norm': 1.3687901496887207, 'learning_rate': 0.00018367346938775512, 'epoch': 0.1}\n",
      "{'loss': 1.1536, 'grad_norm': 1.361364722251892, 'learning_rate': 0.00018326530612244898, 'epoch': 0.1}\n",
      "{'loss': 1.1388, 'grad_norm': 1.3668107986450195, 'learning_rate': 0.00018285714285714286, 'epoch': 0.1}\n",
      "{'loss': 0.9463, 'grad_norm': 1.2242813110351562, 'learning_rate': 0.00018244897959183672, 'epoch': 0.11}\n",
      "{'loss': 1.0094, 'grad_norm': 1.2345843315124512, 'learning_rate': 0.00018204081632653064, 'epoch': 0.11}\n",
      "{'loss': 0.9903, 'grad_norm': 1.28981351852417, 'learning_rate': 0.0001816326530612245, 'epoch': 0.11}\n",
      "{'loss': 1.0778, 'grad_norm': 1.258941888809204, 'learning_rate': 0.00018122448979591838, 'epoch': 0.11}\n",
      "{'loss': 1.1654, 'grad_norm': 1.2433103322982788, 'learning_rate': 0.00018081632653061224, 'epoch': 0.11}\n",
      "{'loss': 0.8155, 'grad_norm': 1.0641800165176392, 'learning_rate': 0.00018040816326530615, 'epoch': 0.12}\n",
      "{'loss': 0.8246, 'grad_norm': 1.1661676168441772, 'learning_rate': 0.00018, 'epoch': 0.12}\n",
      "{'loss': 0.945, 'grad_norm': 1.335849642753601, 'learning_rate': 0.0001795918367346939, 'epoch': 0.12}\n",
      "{'loss': 0.9304, 'grad_norm': 1.301416277885437, 'learning_rate': 0.00017918367346938776, 'epoch': 0.12}\n",
      "{'loss': 0.8599, 'grad_norm': 1.2312991619110107, 'learning_rate': 0.00017877551020408164, 'epoch': 0.12}\n",
      "{'loss': 0.7348, 'grad_norm': 1.4580891132354736, 'learning_rate': 0.00017836734693877553, 'epoch': 0.13}\n",
      "{'loss': 0.7712, 'grad_norm': 1.4865269660949707, 'learning_rate': 0.0001779591836734694, 'epoch': 0.13}\n",
      "{'loss': 0.7844, 'grad_norm': 1.7970188856124878, 'learning_rate': 0.00017755102040816327, 'epoch': 0.13}\n",
      "{'loss': 0.7545, 'grad_norm': 2.0524160861968994, 'learning_rate': 0.00017714285714285713, 'epoch': 0.13}\n",
      "{'loss': 0.7893, 'grad_norm': 1.9787139892578125, 'learning_rate': 0.00017673469387755104, 'epoch': 0.13}\n",
      "{'loss': 0.7613, 'grad_norm': 1.5954309701919556, 'learning_rate': 0.0001763265306122449, 'epoch': 0.14}\n",
      "{'loss': 0.7812, 'grad_norm': 1.2998933792114258, 'learning_rate': 0.0001759183673469388, 'epoch': 0.14}\n",
      "{'loss': 0.8644, 'grad_norm': 1.2457163333892822, 'learning_rate': 0.00017551020408163265, 'epoch': 0.14}\n",
      "{'loss': 0.8178, 'grad_norm': 1.4137492179870605, 'learning_rate': 0.00017510204081632653, 'epoch': 0.14}\n",
      "{'loss': 0.7098, 'grad_norm': 1.3361858129501343, 'learning_rate': 0.00017469387755102042, 'epoch': 0.14}\n",
      "{'loss': 0.6064, 'grad_norm': 1.508482813835144, 'learning_rate': 0.0001742857142857143, 'epoch': 0.15}\n",
      "{'loss': 0.7748, 'grad_norm': 2.1823556423187256, 'learning_rate': 0.00017387755102040816, 'epoch': 0.15}\n",
      "{'loss': 0.8106, 'grad_norm': 1.7439732551574707, 'learning_rate': 0.00017346938775510205, 'epoch': 0.15}\n",
      "{'loss': 0.8259, 'grad_norm': 2.5424787998199463, 'learning_rate': 0.00017306122448979594, 'epoch': 0.15}\n",
      "{'loss': 0.7524, 'grad_norm': 2.3580877780914307, 'learning_rate': 0.00017265306122448982, 'epoch': 0.15}\n",
      "{'loss': 0.7633, 'grad_norm': 2.398009777069092, 'learning_rate': 0.00017224489795918368, 'epoch': 0.16}\n",
      "{'loss': 0.6315, 'grad_norm': 1.9215253591537476, 'learning_rate': 0.00017183673469387757, 'epoch': 0.16}\n",
      "{'loss': 0.92, 'grad_norm': 1.6773892641067505, 'learning_rate': 0.00017142857142857143, 'epoch': 0.16}\n",
      "{'loss': 0.8097, 'grad_norm': 1.5249202251434326, 'learning_rate': 0.0001710204081632653, 'epoch': 0.16}\n",
      "{'loss': 0.7403, 'grad_norm': 1.3939437866210938, 'learning_rate': 0.0001706122448979592, 'epoch': 0.16}\n",
      "{'loss': 0.5261, 'grad_norm': 1.30097496509552, 'learning_rate': 0.00017020408163265306, 'epoch': 0.17}\n",
      "{'loss': 0.7081, 'grad_norm': 1.253184199333191, 'learning_rate': 0.00016979591836734694, 'epoch': 0.17}\n",
      "{'loss': 0.6226, 'grad_norm': 1.3220175504684448, 'learning_rate': 0.00016938775510204083, 'epoch': 0.17}\n",
      "{'loss': 0.5391, 'grad_norm': 1.4712849855422974, 'learning_rate': 0.0001689795918367347, 'epoch': 0.17}\n",
      "{'loss': 0.5618, 'grad_norm': 1.194285273551941, 'learning_rate': 0.00016857142857142857, 'epoch': 0.17}\n",
      "{'loss': 0.9422, 'grad_norm': 1.6835066080093384, 'learning_rate': 0.00016816326530612246, 'epoch': 0.18}\n",
      "{'loss': 0.5833, 'grad_norm': 1.2499001026153564, 'learning_rate': 0.00016775510204081632, 'epoch': 0.18}\n",
      "{'loss': 0.6631, 'grad_norm': 1.3571473360061646, 'learning_rate': 0.00016734693877551023, 'epoch': 0.18}\n",
      "{'loss': 0.6848, 'grad_norm': 1.5133237838745117, 'learning_rate': 0.0001669387755102041, 'epoch': 0.18}\n",
      "{'loss': 0.6431, 'grad_norm': 1.2840297222137451, 'learning_rate': 0.00016653061224489797, 'epoch': 0.18}\n",
      "{'loss': 0.7293, 'grad_norm': 1.52629816532135, 'learning_rate': 0.00016612244897959183, 'epoch': 0.19}\n",
      "{'loss': 0.7261, 'grad_norm': 1.3795360326766968, 'learning_rate': 0.00016571428571428575, 'epoch': 0.19}\n",
      "{'loss': 0.4892, 'grad_norm': 1.276411771774292, 'learning_rate': 0.0001653061224489796, 'epoch': 0.19}\n",
      "{'loss': 0.5391, 'grad_norm': 1.3241254091262817, 'learning_rate': 0.0001648979591836735, 'epoch': 0.19}\n",
      "{'loss': 0.5376, 'grad_norm': 1.0785183906555176, 'learning_rate': 0.00016448979591836735, 'epoch': 0.19}\n",
      "{'loss': 0.4512, 'grad_norm': 1.1884292364120483, 'learning_rate': 0.00016408163265306124, 'epoch': 0.2}\n",
      "{'loss': 0.469, 'grad_norm': 1.293128252029419, 'learning_rate': 0.00016367346938775512, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5571, 'grad_norm': 1.250440001487732, 'learning_rate': 0.00016326530612244898, 'epoch': 0.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b542be3cf9f7472284d9426b7d657e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6914978623390198, 'eval_runtime': 358.3243, 'eval_samples_per_second': 0.558, 'eval_steps_per_second': 0.558, 'epoch': 0.2}\n",
      "{'loss': 0.9285, 'grad_norm': 1.285718560218811, 'learning_rate': 0.00016285714285714287, 'epoch': 0.2}\n",
      "{'loss': 0.9195, 'grad_norm': 1.4854040145874023, 'learning_rate': 0.00016244897959183672, 'epoch': 0.2}\n",
      "{'loss': 0.8655, 'grad_norm': 1.4016894102096558, 'learning_rate': 0.0001620408163265306, 'epoch': 0.21}\n",
      "{'loss': 0.7427, 'grad_norm': 1.2824108600616455, 'learning_rate': 0.0001616326530612245, 'epoch': 0.21}\n",
      "{'loss': 0.8597, 'grad_norm': 1.2052674293518066, 'learning_rate': 0.00016122448979591838, 'epoch': 0.21}\n",
      "{'loss': 0.9721, 'grad_norm': 1.2787489891052246, 'learning_rate': 0.00016081632653061224, 'epoch': 0.21}\n",
      "{'loss': 0.633, 'grad_norm': 1.0599462985992432, 'learning_rate': 0.00016040816326530613, 'epoch': 0.21}\n",
      "{'loss': 0.7294, 'grad_norm': 1.1384029388427734, 'learning_rate': 0.00016, 'epoch': 0.22}\n",
      "{'loss': 0.8522, 'grad_norm': 1.152828335762024, 'learning_rate': 0.0001595918367346939, 'epoch': 0.22}\n",
      "{'loss': 0.8112, 'grad_norm': 1.171972632408142, 'learning_rate': 0.00015918367346938776, 'epoch': 0.22}\n",
      "{'loss': 0.6856, 'grad_norm': 1.0692514181137085, 'learning_rate': 0.00015877551020408164, 'epoch': 0.22}\n",
      "{'loss': 0.8253, 'grad_norm': 1.2236886024475098, 'learning_rate': 0.0001583673469387755, 'epoch': 0.22}\n",
      "{'loss': 0.7426, 'grad_norm': 1.157913088798523, 'learning_rate': 0.00015795918367346942, 'epoch': 0.23}\n",
      "{'loss': 0.7434, 'grad_norm': 1.339104175567627, 'learning_rate': 0.00015755102040816327, 'epoch': 0.23}\n",
      "{'loss': 0.6583, 'grad_norm': 1.194064974784851, 'learning_rate': 0.00015714285714285716, 'epoch': 0.23}\n",
      "{'loss': 0.7737, 'grad_norm': 1.310426115989685, 'learning_rate': 0.00015673469387755102, 'epoch': 0.23}\n",
      "{'loss': 0.7498, 'grad_norm': 1.2456779479980469, 'learning_rate': 0.0001563265306122449, 'epoch': 0.23}\n",
      "{'loss': 0.8274, 'grad_norm': 1.237447738647461, 'learning_rate': 0.0001559183673469388, 'epoch': 0.24}\n",
      "{'loss': 0.7118, 'grad_norm': 1.180824875831604, 'learning_rate': 0.00015551020408163265, 'epoch': 0.24}\n",
      "{'loss': 0.7465, 'grad_norm': 1.1296660900115967, 'learning_rate': 0.00015510204081632654, 'epoch': 0.24}\n",
      "{'loss': 0.6111, 'grad_norm': 1.0452523231506348, 'learning_rate': 0.0001546938775510204, 'epoch': 0.24}\n",
      "{'loss': 0.8201, 'grad_norm': 1.2492966651916504, 'learning_rate': 0.0001542857142857143, 'epoch': 0.24}\n",
      "{'loss': 0.5859, 'grad_norm': 1.2684577703475952, 'learning_rate': 0.00015387755102040817, 'epoch': 0.25}\n",
      "{'loss': 0.6762, 'grad_norm': 1.1288257837295532, 'learning_rate': 0.00015346938775510205, 'epoch': 0.25}\n",
      "{'loss': 0.6092, 'grad_norm': 1.1300617456436157, 'learning_rate': 0.0001530612244897959, 'epoch': 0.25}\n",
      "{'loss': 0.5808, 'grad_norm': 1.1105822324752808, 'learning_rate': 0.00015265306122448982, 'epoch': 0.25}\n",
      "{'loss': 0.8578, 'grad_norm': 1.4554485082626343, 'learning_rate': 0.00015224489795918368, 'epoch': 0.25}\n",
      "{'loss': 0.5782, 'grad_norm': 1.1811182498931885, 'learning_rate': 0.00015183673469387757, 'epoch': 0.26}\n",
      "{'loss': 0.5446, 'grad_norm': 1.1131656169891357, 'learning_rate': 0.00015142857142857143, 'epoch': 0.26}\n",
      "{'loss': 0.5713, 'grad_norm': 1.1619412899017334, 'learning_rate': 0.0001510204081632653, 'epoch': 0.26}\n",
      "{'loss': 0.6518, 'grad_norm': 1.1569100618362427, 'learning_rate': 0.0001506122448979592, 'epoch': 0.26}\n",
      "{'loss': 0.7867, 'grad_norm': 1.3462094068527222, 'learning_rate': 0.00015020408163265306, 'epoch': 0.26}\n",
      "{'loss': 0.5092, 'grad_norm': 1.0504542589187622, 'learning_rate': 0.00014979591836734694, 'epoch': 0.27}\n",
      "{'loss': 0.6822, 'grad_norm': 1.2575180530548096, 'learning_rate': 0.00014938775510204083, 'epoch': 0.27}\n",
      "{'loss': 0.813, 'grad_norm': 1.304492473602295, 'learning_rate': 0.00014897959183673472, 'epoch': 0.27}\n",
      "{'loss': 0.6863, 'grad_norm': 1.2156553268432617, 'learning_rate': 0.00014857142857142857, 'epoch': 0.27}\n",
      "{'loss': 0.5866, 'grad_norm': 1.0943571329116821, 'learning_rate': 0.00014816326530612246, 'epoch': 0.27}\n",
      "{'loss': 0.7874, 'grad_norm': 1.280597448348999, 'learning_rate': 0.00014775510204081632, 'epoch': 0.28}\n",
      "{'loss': 0.6296, 'grad_norm': 1.3690394163131714, 'learning_rate': 0.0001473469387755102, 'epoch': 0.28}\n",
      "{'loss': 0.5418, 'grad_norm': 1.259896993637085, 'learning_rate': 0.0001469387755102041, 'epoch': 0.28}\n",
      "{'loss': 0.4561, 'grad_norm': 1.05922269821167, 'learning_rate': 0.00014653061224489798, 'epoch': 0.28}\n",
      "{'loss': 0.514, 'grad_norm': 1.0938923358917236, 'learning_rate': 0.00014612244897959183, 'epoch': 0.28}\n",
      "{'loss': 0.4999, 'grad_norm': 1.132548451423645, 'learning_rate': 0.00014571428571428572, 'epoch': 0.29}\n",
      "{'loss': 0.5477, 'grad_norm': 1.2810291051864624, 'learning_rate': 0.0001453061224489796, 'epoch': 0.29}\n",
      "{'loss': 0.4925, 'grad_norm': 1.0870451927185059, 'learning_rate': 0.0001448979591836735, 'epoch': 0.29}\n",
      "{'loss': 0.5432, 'grad_norm': 1.4217647314071655, 'learning_rate': 0.00014448979591836735, 'epoch': 0.29}\n",
      "{'loss': 0.5029, 'grad_norm': 1.0923045873641968, 'learning_rate': 0.00014408163265306124, 'epoch': 0.29}\n",
      "{'loss': 0.5439, 'grad_norm': 1.3003674745559692, 'learning_rate': 0.0001436734693877551, 'epoch': 0.3}\n",
      "{'loss': 0.489, 'grad_norm': 1.4087225198745728, 'learning_rate': 0.00014326530612244898, 'epoch': 0.3}\n",
      "{'loss': 0.4857, 'grad_norm': 1.4296324253082275, 'learning_rate': 0.00014285714285714287, 'epoch': 0.3}\n",
      "{'loss': 0.9421, 'grad_norm': 1.4087209701538086, 'learning_rate': 0.00014244897959183673, 'epoch': 0.3}\n",
      "{'loss': 0.8507, 'grad_norm': 1.1734493970870972, 'learning_rate': 0.0001420408163265306, 'epoch': 0.3}\n",
      "{'loss': 0.5981, 'grad_norm': 1.0121209621429443, 'learning_rate': 0.0001416326530612245, 'epoch': 0.31}\n",
      "{'loss': 0.7677, 'grad_norm': 1.1197535991668701, 'learning_rate': 0.00014122448979591838, 'epoch': 0.31}\n",
      "{'loss': 0.5745, 'grad_norm': 1.1241317987442017, 'learning_rate': 0.00014081632653061224, 'epoch': 0.31}\n",
      "{'loss': 0.7696, 'grad_norm': 1.3077164888381958, 'learning_rate': 0.00014040816326530613, 'epoch': 0.31}\n",
      "{'loss': 0.8481, 'grad_norm': 1.1703391075134277, 'learning_rate': 0.00014, 'epoch': 0.31}\n",
      "{'loss': 0.7201, 'grad_norm': 1.156428575515747, 'learning_rate': 0.0001395918367346939, 'epoch': 0.32}\n",
      "{'loss': 0.7677, 'grad_norm': 1.2199691534042358, 'learning_rate': 0.00013918367346938776, 'epoch': 0.32}\n",
      "{'loss': 0.6759, 'grad_norm': 1.2223615646362305, 'learning_rate': 0.00013877551020408165, 'epoch': 0.32}\n",
      "{'loss': 0.6319, 'grad_norm': 1.132553219795227, 'learning_rate': 0.0001383673469387755, 'epoch': 0.32}\n",
      "{'loss': 0.6623, 'grad_norm': 1.1128844022750854, 'learning_rate': 0.00013795918367346942, 'epoch': 0.32}\n",
      "{'loss': 0.6737, 'grad_norm': 1.2705485820770264, 'learning_rate': 0.00013755102040816328, 'epoch': 0.33}\n",
      "{'loss': 0.5476, 'grad_norm': 1.1449209451675415, 'learning_rate': 0.00013714285714285716, 'epoch': 0.33}\n",
      "{'loss': 0.7355, 'grad_norm': 1.337320327758789, 'learning_rate': 0.00013673469387755102, 'epoch': 0.33}\n",
      "{'loss': 0.8282, 'grad_norm': 1.241848111152649, 'learning_rate': 0.0001363265306122449, 'epoch': 0.33}\n",
      "{'loss': 0.6852, 'grad_norm': 1.3000966310501099, 'learning_rate': 0.0001359183673469388, 'epoch': 0.33}\n",
      "{'loss': 0.7293, 'grad_norm': 1.1643621921539307, 'learning_rate': 0.00013551020408163265, 'epoch': 0.34}\n",
      "{'loss': 0.605, 'grad_norm': 1.1273959875106812, 'learning_rate': 0.00013510204081632654, 'epoch': 0.34}\n",
      "{'loss': 0.5861, 'grad_norm': 1.094878077507019, 'learning_rate': 0.0001346938775510204, 'epoch': 0.34}\n",
      "{'loss': 0.5747, 'grad_norm': 1.1430145502090454, 'learning_rate': 0.00013428571428571428, 'epoch': 0.34}\n",
      "{'loss': 0.7095, 'grad_norm': 1.3208497762680054, 'learning_rate': 0.00013387755102040817, 'epoch': 0.34}\n",
      "{'loss': 0.6578, 'grad_norm': 1.1248303651809692, 'learning_rate': 0.00013346938775510205, 'epoch': 0.35}\n",
      "{'loss': 0.5852, 'grad_norm': 1.1641128063201904, 'learning_rate': 0.0001330612244897959, 'epoch': 0.35}\n",
      "{'loss': 0.5866, 'grad_norm': 1.2890408039093018, 'learning_rate': 0.0001326530612244898, 'epoch': 0.35}\n",
      "{'loss': 0.6988, 'grad_norm': 1.374039888381958, 'learning_rate': 0.00013224489795918368, 'epoch': 0.35}\n",
      "{'loss': 0.5282, 'grad_norm': 1.1934934854507446, 'learning_rate': 0.00013183673469387757, 'epoch': 0.35}\n",
      "{'loss': 0.5933, 'grad_norm': 1.2168339490890503, 'learning_rate': 0.00013142857142857143, 'epoch': 0.36}\n",
      "{'loss': 0.6209, 'grad_norm': 1.2835277318954468, 'learning_rate': 0.00013102040816326531, 'epoch': 0.36}\n",
      "{'loss': 0.5952, 'grad_norm': 1.2898088693618774, 'learning_rate': 0.00013061224489795917, 'epoch': 0.36}\n",
      "{'loss': 0.5144, 'grad_norm': 1.0997027158737183, 'learning_rate': 0.00013020408163265309, 'epoch': 0.36}\n",
      "{'loss': 0.5353, 'grad_norm': 1.069926142692566, 'learning_rate': 0.00012979591836734695, 'epoch': 0.36}\n",
      "{'loss': 0.5155, 'grad_norm': 1.124671459197998, 'learning_rate': 0.00012938775510204083, 'epoch': 0.37}\n",
      "{'loss': 0.7089, 'grad_norm': 1.3119796514511108, 'learning_rate': 0.0001289795918367347, 'epoch': 0.37}\n",
      "{'loss': 0.5927, 'grad_norm': 1.1767178773880005, 'learning_rate': 0.00012857142857142858, 'epoch': 0.37}\n",
      "{'loss': 0.7008, 'grad_norm': 1.2497972249984741, 'learning_rate': 0.00012816326530612246, 'epoch': 0.37}\n",
      "{'loss': 0.5054, 'grad_norm': 1.0622743368148804, 'learning_rate': 0.00012775510204081632, 'epoch': 0.37}\n",
      "{'loss': 0.4688, 'grad_norm': 1.0746222734451294, 'learning_rate': 0.0001273469387755102, 'epoch': 0.38}\n",
      "{'loss': 0.4515, 'grad_norm': 1.036375880241394, 'learning_rate': 0.00012693877551020406, 'epoch': 0.38}\n",
      "{'loss': 0.462, 'grad_norm': 0.9672584533691406, 'learning_rate': 0.00012653061224489798, 'epoch': 0.38}\n",
      "{'loss': 0.4605, 'grad_norm': 1.0968914031982422, 'learning_rate': 0.00012612244897959184, 'epoch': 0.38}\n",
      "{'loss': 0.523, 'grad_norm': 1.0999771356582642, 'learning_rate': 0.00012571428571428572, 'epoch': 0.38}\n",
      "{'loss': 0.6617, 'grad_norm': 1.219409704208374, 'learning_rate': 0.00012530612244897958, 'epoch': 0.39}\n",
      "{'loss': 0.522, 'grad_norm': 1.1051664352416992, 'learning_rate': 0.0001248979591836735, 'epoch': 0.39}\n",
      "{'loss': 0.4574, 'grad_norm': 1.2054152488708496, 'learning_rate': 0.00012448979591836735, 'epoch': 0.39}\n",
      "{'loss': 0.4224, 'grad_norm': 1.0843720436096191, 'learning_rate': 0.00012408163265306124, 'epoch': 0.39}\n",
      "{'loss': 0.4194, 'grad_norm': 1.0333552360534668, 'learning_rate': 0.0001236734693877551, 'epoch': 0.39}\n",
      "{'loss': 0.5737, 'grad_norm': 1.3907244205474854, 'learning_rate': 0.00012326530612244898, 'epoch': 0.4}\n",
      "{'loss': 0.4064, 'grad_norm': 1.1685330867767334, 'learning_rate': 0.00012285714285714287, 'epoch': 0.4}\n",
      "{'loss': 0.3831, 'grad_norm': 1.0628163814544678, 'learning_rate': 0.00012244897959183676, 'epoch': 0.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4104a9bb0f5d4f1483fd958eea98d119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6227226257324219, 'eval_runtime': 358.4178, 'eval_samples_per_second': 0.558, 'eval_steps_per_second': 0.558, 'epoch': 0.4}\n",
      "{'loss': 0.7704, 'grad_norm': 0.9951809644699097, 'learning_rate': 0.00012204081632653061, 'epoch': 0.4}\n",
      "{'loss': 0.8358, 'grad_norm': 1.064465045928955, 'learning_rate': 0.00012163265306122449, 'epoch': 0.4}\n",
      "{'loss': 0.6479, 'grad_norm': 1.0055352449417114, 'learning_rate': 0.00012122448979591839, 'epoch': 0.41}\n",
      "{'loss': 0.5478, 'grad_norm': 0.9605713486671448, 'learning_rate': 0.00012081632653061226, 'epoch': 0.41}\n",
      "{'loss': 0.8093, 'grad_norm': 1.0541173219680786, 'learning_rate': 0.00012040816326530613, 'epoch': 0.41}\n",
      "{'loss': 0.8588, 'grad_norm': 1.2313696146011353, 'learning_rate': 0.00012, 'epoch': 0.41}\n",
      "{'loss': 0.7009, 'grad_norm': 1.2183709144592285, 'learning_rate': 0.00011959183673469388, 'epoch': 0.41}\n",
      "{'loss': 0.8127, 'grad_norm': 1.1026209592819214, 'learning_rate': 0.00011918367346938777, 'epoch': 0.42}\n",
      "{'loss': 0.6524, 'grad_norm': 0.9918947219848633, 'learning_rate': 0.00011877551020408165, 'epoch': 0.42}\n",
      "{'loss': 0.5961, 'grad_norm': 1.2349674701690674, 'learning_rate': 0.00011836734693877552, 'epoch': 0.42}\n",
      "{'loss': 0.605, 'grad_norm': 0.9208088517189026, 'learning_rate': 0.00011795918367346939, 'epoch': 0.42}\n",
      "{'loss': 0.8659, 'grad_norm': 1.0908703804016113, 'learning_rate': 0.00011755102040816328, 'epoch': 0.42}\n",
      "{'loss': 0.584, 'grad_norm': 1.0569638013839722, 'learning_rate': 0.00011714285714285715, 'epoch': 0.43}\n",
      "{'loss': 0.5835, 'grad_norm': 1.0502229928970337, 'learning_rate': 0.00011673469387755102, 'epoch': 0.43}\n",
      "{'loss': 0.7601, 'grad_norm': 1.1070952415466309, 'learning_rate': 0.0001163265306122449, 'epoch': 0.43}\n",
      "{'loss': 0.7049, 'grad_norm': 1.138940453529358, 'learning_rate': 0.00011591836734693877, 'epoch': 0.43}\n",
      "{'loss': 0.7151, 'grad_norm': 1.2164870500564575, 'learning_rate': 0.00011551020408163267, 'epoch': 0.43}\n",
      "{'loss': 0.6073, 'grad_norm': 1.1027196645736694, 'learning_rate': 0.00011510204081632654, 'epoch': 0.44}\n",
      "{'loss': 0.6519, 'grad_norm': 1.1714143753051758, 'learning_rate': 0.00011469387755102041, 'epoch': 0.44}\n",
      "{'loss': 0.6151, 'grad_norm': 1.1817066669464111, 'learning_rate': 0.00011428571428571428, 'epoch': 0.44}\n",
      "{'loss': 0.6963, 'grad_norm': 1.1342016458511353, 'learning_rate': 0.00011387755102040818, 'epoch': 0.44}\n",
      "{'loss': 0.6697, 'grad_norm': 1.2689576148986816, 'learning_rate': 0.00011346938775510206, 'epoch': 0.44}\n",
      "{'loss': 0.538, 'grad_norm': 1.0451868772506714, 'learning_rate': 0.00011306122448979593, 'epoch': 0.45}\n",
      "{'loss': 0.8231, 'grad_norm': 1.3539859056472778, 'learning_rate': 0.0001126530612244898, 'epoch': 0.45}\n",
      "{'loss': 0.7091, 'grad_norm': 1.2554268836975098, 'learning_rate': 0.00011224489795918367, 'epoch': 0.45}\n",
      "{'loss': 0.6048, 'grad_norm': 0.9884517788887024, 'learning_rate': 0.00011183673469387757, 'epoch': 0.45}\n",
      "{'loss': 0.591, 'grad_norm': 1.1335546970367432, 'learning_rate': 0.00011142857142857144, 'epoch': 0.45}\n",
      "{'loss': 0.5153, 'grad_norm': 1.0674809217453003, 'learning_rate': 0.00011102040816326532, 'epoch': 0.46}\n",
      "{'loss': 0.6112, 'grad_norm': 1.092908501625061, 'learning_rate': 0.00011061224489795919, 'epoch': 0.46}\n",
      "{'loss': 0.6596, 'grad_norm': 1.1047531366348267, 'learning_rate': 0.00011020408163265306, 'epoch': 0.46}\n",
      "{'loss': 0.4819, 'grad_norm': 1.2363831996917725, 'learning_rate': 0.00010979591836734695, 'epoch': 0.46}\n",
      "{'loss': 0.4156, 'grad_norm': 1.0477012395858765, 'learning_rate': 0.00010938775510204082, 'epoch': 0.46}\n",
      "{'loss': 0.7501, 'grad_norm': 1.1614351272583008, 'learning_rate': 0.00010897959183673469, 'epoch': 0.47}\n",
      "{'loss': 0.5037, 'grad_norm': 1.0866683721542358, 'learning_rate': 0.00010857142857142856, 'epoch': 0.47}\n",
      "{'loss': 0.5856, 'grad_norm': 1.319706916809082, 'learning_rate': 0.00010816326530612246, 'epoch': 0.47}\n",
      "{'loss': 0.4711, 'grad_norm': 1.1836103200912476, 'learning_rate': 0.00010775510204081634, 'epoch': 0.47}\n",
      "{'loss': 0.5584, 'grad_norm': 1.2420088052749634, 'learning_rate': 0.00010734693877551021, 'epoch': 0.47}\n",
      "{'loss': 0.5376, 'grad_norm': 1.0502358675003052, 'learning_rate': 0.00010693877551020408, 'epoch': 0.48}\n",
      "{'loss': 0.6625, 'grad_norm': 1.3465555906295776, 'learning_rate': 0.00010653061224489795, 'epoch': 0.48}\n",
      "{'loss': 0.5537, 'grad_norm': 1.087052345275879, 'learning_rate': 0.00010612244897959185, 'epoch': 0.48}\n",
      "{'loss': 0.5789, 'grad_norm': 1.2356722354888916, 'learning_rate': 0.00010571428571428572, 'epoch': 0.48}\n",
      "{'loss': 0.5459, 'grad_norm': 1.1388030052185059, 'learning_rate': 0.0001053061224489796, 'epoch': 0.48}\n",
      "{'loss': 0.5951, 'grad_norm': 1.2492281198501587, 'learning_rate': 0.00010489795918367347, 'epoch': 0.49}\n",
      "{'loss': 0.4398, 'grad_norm': 0.9799475073814392, 'learning_rate': 0.00010448979591836735, 'epoch': 0.49}\n",
      "{'loss': 0.7419, 'grad_norm': 1.3964741230010986, 'learning_rate': 0.00010408163265306123, 'epoch': 0.49}\n",
      "{'loss': 0.4915, 'grad_norm': 1.0605449676513672, 'learning_rate': 0.00010367346938775511, 'epoch': 0.49}\n",
      "{'loss': 0.4526, 'grad_norm': 1.0726279020309448, 'learning_rate': 0.00010326530612244899, 'epoch': 0.49}\n",
      "{'loss': 0.4548, 'grad_norm': 1.0602902173995972, 'learning_rate': 0.00010285714285714286, 'epoch': 0.5}\n",
      "{'loss': 0.5204, 'grad_norm': 1.327796459197998, 'learning_rate': 0.00010244897959183674, 'epoch': 0.5}\n",
      "{'loss': 0.4963, 'grad_norm': 1.3562582731246948, 'learning_rate': 0.00010204081632653062, 'epoch': 0.5}\n",
      "{'loss': 0.8853, 'grad_norm': 1.0923632383346558, 'learning_rate': 0.00010163265306122449, 'epoch': 0.5}\n",
      "{'loss': 0.846, 'grad_norm': 1.0981048345565796, 'learning_rate': 0.00010122448979591836, 'epoch': 0.5}\n",
      "{'loss': 0.8311, 'grad_norm': 1.0993472337722778, 'learning_rate': 0.00010081632653061226, 'epoch': 0.51}\n",
      "{'loss': 0.7403, 'grad_norm': 1.0704691410064697, 'learning_rate': 0.00010040816326530613, 'epoch': 0.51}\n",
      "{'loss': 0.8091, 'grad_norm': 1.1419013738632202, 'learning_rate': 0.0001, 'epoch': 0.51}\n",
      "{'loss': 0.6389, 'grad_norm': 1.0009773969650269, 'learning_rate': 9.959183673469388e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7017, 'grad_norm': 0.9358514547348022, 'learning_rate': 9.918367346938776e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6452, 'grad_norm': 0.9985034465789795, 'learning_rate': 9.877551020408164e-05, 'epoch': 0.52}\n",
      "{'loss': 0.659, 'grad_norm': 0.930541455745697, 'learning_rate': 9.836734693877552e-05, 'epoch': 0.52}\n",
      "{'loss': 0.568, 'grad_norm': 0.9597124457359314, 'learning_rate': 9.79591836734694e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6071, 'grad_norm': 1.0624315738677979, 'learning_rate': 9.755102040816328e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6854, 'grad_norm': 1.0694928169250488, 'learning_rate': 9.714285714285715e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4677, 'grad_norm': 0.8507325053215027, 'learning_rate': 9.673469387755102e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6421, 'grad_norm': 1.1145191192626953, 'learning_rate': 9.63265306122449e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5964, 'grad_norm': 1.0172008275985718, 'learning_rate': 9.591836734693878e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5099, 'grad_norm': 1.0718845129013062, 'learning_rate': 9.551020408163265e-05, 'epoch': 0.53}\n",
      "{'loss': 0.4889, 'grad_norm': 0.9863401651382446, 'learning_rate': 9.510204081632653e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5979, 'grad_norm': 1.0273619890213013, 'learning_rate': 9.469387755102041e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7201, 'grad_norm': 1.0974650382995605, 'learning_rate': 9.428571428571429e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5067, 'grad_norm': 1.0092651844024658, 'learning_rate': 9.387755102040817e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5338, 'grad_norm': 0.988774836063385, 'learning_rate': 9.346938775510204e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6268, 'grad_norm': 1.0669997930526733, 'learning_rate': 9.306122448979592e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7593, 'grad_norm': 1.1864112615585327, 'learning_rate': 9.26530612244898e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5796, 'grad_norm': 1.057950735092163, 'learning_rate': 9.224489795918367e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5883, 'grad_norm': 1.1102707386016846, 'learning_rate': 9.183673469387756e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5814, 'grad_norm': 1.056233286857605, 'learning_rate': 9.142857142857143e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6261, 'grad_norm': 1.135158896446228, 'learning_rate': 9.102040816326532e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5402, 'grad_norm': 1.006543755531311, 'learning_rate': 9.061224489795919e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5668, 'grad_norm': 1.0874927043914795, 'learning_rate': 9.020408163265308e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5204, 'grad_norm': 1.1070858240127563, 'learning_rate': 8.979591836734695e-05, 'epoch': 0.56}\n",
      "{'loss': 0.621, 'grad_norm': 1.1708534955978394, 'learning_rate': 8.938775510204082e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6407, 'grad_norm': 1.22721266746521, 'learning_rate': 8.89795918367347e-05, 'epoch': 0.56}\n",
      "{'loss': 0.603, 'grad_norm': 1.1333273649215698, 'learning_rate': 8.857142857142857e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6228, 'grad_norm': 1.1427301168441772, 'learning_rate': 8.816326530612245e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7473, 'grad_norm': 1.1530245542526245, 'learning_rate': 8.775510204081632e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5977, 'grad_norm': 1.0539342164993286, 'learning_rate': 8.734693877551021e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5508, 'grad_norm': 1.0251470804214478, 'learning_rate': 8.693877551020408e-05, 'epoch': 0.57}\n",
      "{'loss': 0.4781, 'grad_norm': 0.9554804563522339, 'learning_rate': 8.653061224489797e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6168, 'grad_norm': 1.0263926982879639, 'learning_rate': 8.612244897959184e-05, 'epoch': 0.58}\n",
      "{'loss': 0.4958, 'grad_norm': 1.0628607273101807, 'learning_rate': 8.571428571428571e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5074, 'grad_norm': 0.990233838558197, 'learning_rate': 8.53061224489796e-05, 'epoch': 0.58}\n",
      "{'loss': 0.4234, 'grad_norm': 0.9328935742378235, 'learning_rate': 8.489795918367347e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3956, 'grad_norm': 0.990553081035614, 'learning_rate': 8.448979591836736e-05, 'epoch': 0.59}\n",
      "{'loss': 0.679, 'grad_norm': 1.2510688304901123, 'learning_rate': 8.408163265306123e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5357, 'grad_norm': 1.1132787466049194, 'learning_rate': 8.367346938775511e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5228, 'grad_norm': 1.1589007377624512, 'learning_rate': 8.326530612244899e-05, 'epoch': 0.59}\n",
      "{'loss': 0.4228, 'grad_norm': 1.0197923183441162, 'learning_rate': 8.285714285714287e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5257, 'grad_norm': 1.2437859773635864, 'learning_rate': 8.244897959183675e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3953, 'grad_norm': 1.0489392280578613, 'learning_rate': 8.204081632653062e-05, 'epoch': 0.6}\n",
      "{'loss': 0.4223, 'grad_norm': 1.1630388498306274, 'learning_rate': 8.163265306122449e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9b3c4a1b784af6add9e64d98cc348a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5676608085632324, 'eval_runtime': 358.3484, 'eval_samples_per_second': 0.558, 'eval_steps_per_second': 0.558, 'epoch': 0.6}\n",
      "{'loss': 0.8192, 'grad_norm': 1.0733829736709595, 'learning_rate': 8.122448979591836e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5838, 'grad_norm': 1.2251416444778442, 'learning_rate': 8.081632653061225e-05, 'epoch': 0.6}\n",
      "{'loss': 0.598, 'grad_norm': 1.0642205476760864, 'learning_rate': 8.040816326530612e-05, 'epoch': 0.61}\n",
      "{'loss': 0.8126, 'grad_norm': 1.1311894655227661, 'learning_rate': 8e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5604, 'grad_norm': 1.0192772150039673, 'learning_rate': 7.959183673469388e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6134, 'grad_norm': 1.087037444114685, 'learning_rate': 7.918367346938775e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6472, 'grad_norm': 1.201444387435913, 'learning_rate': 7.877551020408164e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5207, 'grad_norm': 1.0680526494979858, 'learning_rate': 7.836734693877551e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5491, 'grad_norm': 1.1610106229782104, 'learning_rate': 7.79591836734694e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5386, 'grad_norm': 1.0565866231918335, 'learning_rate': 7.755102040816327e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5113, 'grad_norm': 1.1853482723236084, 'learning_rate': 7.714285714285715e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5547, 'grad_norm': 1.0526798963546753, 'learning_rate': 7.673469387755103e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7421, 'grad_norm': 1.2136861085891724, 'learning_rate': 7.632653061224491e-05, 'epoch': 0.63}\n",
      "{'loss': 0.613, 'grad_norm': 1.0722031593322754, 'learning_rate': 7.591836734693878e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6317, 'grad_norm': 1.0434659719467163, 'learning_rate': 7.551020408163266e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6331, 'grad_norm': 1.116045355796814, 'learning_rate': 7.510204081632653e-05, 'epoch': 0.63}\n",
      "{'loss': 0.625, 'grad_norm': 1.0093369483947754, 'learning_rate': 7.469387755102041e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5736, 'grad_norm': 1.099471926689148, 'learning_rate': 7.428571428571429e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5136, 'grad_norm': 1.059276819229126, 'learning_rate': 7.387755102040816e-05, 'epoch': 0.64}\n",
      "{'loss': 0.505, 'grad_norm': 0.8571754097938538, 'learning_rate': 7.346938775510205e-05, 'epoch': 0.64}\n",
      "{'loss': 0.581, 'grad_norm': 1.0299710035324097, 'learning_rate': 7.306122448979592e-05, 'epoch': 0.64}\n",
      "{'loss': 0.4672, 'grad_norm': 0.928047776222229, 'learning_rate': 7.26530612244898e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5159, 'grad_norm': 0.9873030185699463, 'learning_rate': 7.224489795918368e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5927, 'grad_norm': 1.1046358346939087, 'learning_rate': 7.183673469387755e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5501, 'grad_norm': 1.0168603658676147, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6107, 'grad_norm': 1.2391879558563232, 'learning_rate': 7.10204081632653e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5599, 'grad_norm': 1.0948212146759033, 'learning_rate': 7.061224489795919e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6231, 'grad_norm': 1.1915632486343384, 'learning_rate': 7.020408163265306e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5994, 'grad_norm': 1.1555672883987427, 'learning_rate': 6.979591836734695e-05, 'epoch': 0.66}\n",
      "{'loss': 0.542, 'grad_norm': 1.0656540393829346, 'learning_rate': 6.938775510204082e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5108, 'grad_norm': 1.049917459487915, 'learning_rate': 6.897959183673471e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5116, 'grad_norm': 1.0506463050842285, 'learning_rate': 6.857142857142858e-05, 'epoch': 0.66}\n",
      "{'loss': 0.4742, 'grad_norm': 0.9868191480636597, 'learning_rate': 6.816326530612245e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5086, 'grad_norm': 0.9977805018424988, 'learning_rate': 6.775510204081633e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5107, 'grad_norm': 1.2208715677261353, 'learning_rate': 6.73469387755102e-05, 'epoch': 0.67}\n",
      "{'loss': 0.4118, 'grad_norm': 0.9908266067504883, 'learning_rate': 6.693877551020408e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5498, 'grad_norm': 1.2065802812576294, 'learning_rate': 6.653061224489796e-05, 'epoch': 0.67}\n",
      "{'loss': 0.4481, 'grad_norm': 0.9692352414131165, 'learning_rate': 6.612244897959184e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5104, 'grad_norm': 1.2215118408203125, 'learning_rate': 6.571428571428571e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3988, 'grad_norm': 1.0315485000610352, 'learning_rate': 6.530612244897959e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3903, 'grad_norm': 0.9954822659492493, 'learning_rate': 6.489795918367347e-05, 'epoch': 0.68}\n",
      "{'loss': 0.4225, 'grad_norm': 1.028591275215149, 'learning_rate': 6.448979591836734e-05, 'epoch': 0.68}\n",
      "{'loss': 0.4568, 'grad_norm': 1.26265549659729, 'learning_rate': 6.408163265306123e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5895, 'grad_norm': 1.243048906326294, 'learning_rate': 6.36734693877551e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3886, 'grad_norm': 1.0770496129989624, 'learning_rate': 6.326530612244899e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4014, 'grad_norm': 1.0731669664382935, 'learning_rate': 6.285714285714286e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4968, 'grad_norm': 1.2957075834274292, 'learning_rate': 6.244897959183675e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4034, 'grad_norm': 1.2215478420257568, 'learning_rate': 6.204081632653062e-05, 'epoch': 0.7}\n",
      "{'loss': 0.348, 'grad_norm': 1.1734063625335693, 'learning_rate': 6.163265306122449e-05, 'epoch': 0.7}\n",
      "{'loss': 0.4461, 'grad_norm': 1.3158031702041626, 'learning_rate': 6.122448979591838e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7813, 'grad_norm': 0.9314399361610413, 'learning_rate': 6.081632653061224e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8057, 'grad_norm': 1.0534895658493042, 'learning_rate': 6.040816326530613e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6936, 'grad_norm': 1.0189576148986816, 'learning_rate': 6e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5534, 'grad_norm': 0.9195781946182251, 'learning_rate': 5.959183673469389e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7736, 'grad_norm': 1.1241058111190796, 'learning_rate': 5.918367346938776e-05, 'epoch': 0.71}\n",
      "{'loss': 0.608, 'grad_norm': 1.0105897188186646, 'learning_rate': 5.877551020408164e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6581, 'grad_norm': 1.202109932899475, 'learning_rate': 5.836734693877551e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5795, 'grad_norm': 1.054945468902588, 'learning_rate': 5.7959183673469384e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7191, 'grad_norm': 1.1463823318481445, 'learning_rate': 5.755102040816327e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6819, 'grad_norm': 1.06001877784729, 'learning_rate': 5.714285714285714e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7054, 'grad_norm': 1.075537085533142, 'learning_rate': 5.673469387755103e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6309, 'grad_norm': 1.1968934535980225, 'learning_rate': 5.63265306122449e-05, 'epoch': 0.72}\n",
      "{'loss': 0.4804, 'grad_norm': 1.0074154138565063, 'learning_rate': 5.5918367346938786e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6576, 'grad_norm': 1.083520770072937, 'learning_rate': 5.551020408163266e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7116, 'grad_norm': 1.1918556690216064, 'learning_rate': 5.510204081632653e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5167, 'grad_norm': 1.0720127820968628, 'learning_rate': 5.469387755102041e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5893, 'grad_norm': 1.1840736865997314, 'learning_rate': 5.428571428571428e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5901, 'grad_norm': 1.0340381860733032, 'learning_rate': 5.387755102040817e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6378, 'grad_norm': 1.1372584104537964, 'learning_rate': 5.346938775510204e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5597, 'grad_norm': 0.9805184602737427, 'learning_rate': 5.3061224489795926e-05, 'epoch': 0.74}\n",
      "{'loss': 0.579, 'grad_norm': 1.0917809009552002, 'learning_rate': 5.26530612244898e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6074, 'grad_norm': 1.1636319160461426, 'learning_rate': 5.224489795918368e-05, 'epoch': 0.74}\n",
      "{'loss': 0.4281, 'grad_norm': 0.9092609882354736, 'learning_rate': 5.1836734693877557e-05, 'epoch': 0.75}\n",
      "{'loss': 0.4878, 'grad_norm': 1.0274145603179932, 'learning_rate': 5.142857142857143e-05, 'epoch': 0.75}\n",
      "{'loss': 0.4684, 'grad_norm': 0.9151191711425781, 'learning_rate': 5.102040816326531e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3773, 'grad_norm': 1.0057928562164307, 'learning_rate': 5.061224489795918e-05, 'epoch': 0.75}\n",
      "{'loss': 0.4685, 'grad_norm': 1.028220295906067, 'learning_rate': 5.0204081632653066e-05, 'epoch': 0.75}\n",
      "{'loss': 0.5502, 'grad_norm': 1.1371216773986816, 'learning_rate': 4.979591836734694e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5018, 'grad_norm': 1.1840280294418335, 'learning_rate': 4.938775510204082e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5379, 'grad_norm': 1.1162972450256348, 'learning_rate': 4.89795918367347e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7025, 'grad_norm': 1.4323261976242065, 'learning_rate': 4.8571428571428576e-05, 'epoch': 0.76}\n",
      "{'loss': 0.4956, 'grad_norm': 1.050047516822815, 'learning_rate': 4.816326530612245e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7721, 'grad_norm': 1.2802423238754272, 'learning_rate': 4.775510204081633e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4675, 'grad_norm': 1.032461166381836, 'learning_rate': 4.7346938775510206e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4691, 'grad_norm': 1.0966591835021973, 'learning_rate': 4.6938775510204086e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7454, 'grad_norm': 1.8587498664855957, 'learning_rate': 4.653061224489796e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4911, 'grad_norm': 1.1408047676086426, 'learning_rate': 4.612244897959184e-05, 'epoch': 0.77}\n",
      "{'loss': 0.409, 'grad_norm': 1.0839987993240356, 'learning_rate': 4.5714285714285716e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5639, 'grad_norm': 1.222922921180725, 'learning_rate': 4.5306122448979595e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4587, 'grad_norm': 1.1372876167297363, 'learning_rate': 4.4897959183673474e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4658, 'grad_norm': 0.9990001916885376, 'learning_rate': 4.448979591836735e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5236, 'grad_norm': 1.3007969856262207, 'learning_rate': 4.4081632653061226e-05, 'epoch': 0.78}\n",
      "{'loss': 0.3934, 'grad_norm': 1.054037094116211, 'learning_rate': 4.3673469387755105e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3705, 'grad_norm': 0.9386894702911377, 'learning_rate': 4.3265306122448984e-05, 'epoch': 0.79}\n",
      "{'loss': 0.548, 'grad_norm': 1.1437032222747803, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.79}\n",
      "{'loss': 0.4583, 'grad_norm': 1.1868014335632324, 'learning_rate': 4.2448979591836735e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3902, 'grad_norm': 0.979773998260498, 'learning_rate': 4.2040816326530615e-05, 'epoch': 0.79}\n",
      "{'loss': 0.4638, 'grad_norm': 1.1944535970687866, 'learning_rate': 4.1632653061224494e-05, 'epoch': 0.8}\n",
      "{'loss': 0.415, 'grad_norm': 1.191264033317566, 'learning_rate': 4.122448979591837e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3222, 'grad_norm': 1.162025809288025, 'learning_rate': 4.0816326530612245e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55511ac163274ac5a45dc8d8653fe49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5340343117713928, 'eval_runtime': 358.3847, 'eval_samples_per_second': 0.558, 'eval_steps_per_second': 0.558, 'epoch': 0.8}\n",
      "{'loss': 0.7614, 'grad_norm': 0.9188481569290161, 'learning_rate': 4.0408163265306124e-05, 'epoch': 0.8}\n",
      "{'loss': 0.784, 'grad_norm': 0.9329474568367004, 'learning_rate': 4e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6955, 'grad_norm': 0.9609838724136353, 'learning_rate': 3.9591836734693876e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5845, 'grad_norm': 1.0288286209106445, 'learning_rate': 3.9183673469387755e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7611, 'grad_norm': 1.1097233295440674, 'learning_rate': 3.8775510204081634e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6164, 'grad_norm': 1.0679646730422974, 'learning_rate': 3.836734693877551e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5526, 'grad_norm': 1.0703438520431519, 'learning_rate': 3.795918367346939e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5348, 'grad_norm': 1.0354113578796387, 'learning_rate': 3.7551020408163264e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6693, 'grad_norm': 1.1544369459152222, 'learning_rate': 3.7142857142857143e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5166, 'grad_norm': 1.001381754875183, 'learning_rate': 3.673469387755102e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6509, 'grad_norm': 1.0665228366851807, 'learning_rate': 3.63265306122449e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8228, 'grad_norm': 1.347633719444275, 'learning_rate': 3.5918367346938774e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5685, 'grad_norm': 1.090424656867981, 'learning_rate': 3.551020408163265e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6386, 'grad_norm': 1.156989574432373, 'learning_rate': 3.510204081632653e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6714, 'grad_norm': 1.2162532806396484, 'learning_rate': 3.469387755102041e-05, 'epoch': 0.83}\n",
      "{'loss': 0.4936, 'grad_norm': 0.997813880443573, 'learning_rate': 3.428571428571429e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8655, 'grad_norm': 1.2804675102233887, 'learning_rate': 3.387755102040816e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6259, 'grad_norm': 1.190431833267212, 'learning_rate': 3.346938775510204e-05, 'epoch': 0.84}\n",
      "{'loss': 0.4494, 'grad_norm': 0.9984094500541687, 'learning_rate': 3.306122448979592e-05, 'epoch': 0.84}\n",
      "{'loss': 0.491, 'grad_norm': 0.9955668449401855, 'learning_rate': 3.265306122448979e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5204, 'grad_norm': 1.0239222049713135, 'learning_rate': 3.224489795918367e-05, 'epoch': 0.84}\n",
      "{'loss': 0.411, 'grad_norm': 0.961209774017334, 'learning_rate': 3.183673469387755e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5317, 'grad_norm': 1.0525649785995483, 'learning_rate': 3.142857142857143e-05, 'epoch': 0.85}\n",
      "{'loss': 0.4732, 'grad_norm': 1.0592066049575806, 'learning_rate': 3.102040816326531e-05, 'epoch': 0.85}\n",
      "{'loss': 0.632, 'grad_norm': 1.1573419570922852, 'learning_rate': 3.061224489795919e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5857, 'grad_norm': 1.113239049911499, 'learning_rate': 3.0204081632653065e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5656, 'grad_norm': 1.0751336812973022, 'learning_rate': 2.9795918367346944e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5165, 'grad_norm': 0.9827169179916382, 'learning_rate': 2.938775510204082e-05, 'epoch': 0.86}\n",
      "{'loss': 0.566, 'grad_norm': 1.182431697845459, 'learning_rate': 2.8979591836734692e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6283, 'grad_norm': 1.0906920433044434, 'learning_rate': 2.857142857142857e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5291, 'grad_norm': 1.1064413785934448, 'learning_rate': 2.816326530612245e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5522, 'grad_norm': 1.1615674495697021, 'learning_rate': 2.775510204081633e-05, 'epoch': 0.86}\n",
      "{'loss': 0.4853, 'grad_norm': 0.9837641716003418, 'learning_rate': 2.7346938775510205e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3728, 'grad_norm': 0.9329716563224792, 'learning_rate': 2.6938775510204084e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5611, 'grad_norm': 1.1203937530517578, 'learning_rate': 2.6530612244897963e-05, 'epoch': 0.87}\n",
      "{'loss': 0.4816, 'grad_norm': 1.0284554958343506, 'learning_rate': 2.612244897959184e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5122, 'grad_norm': 1.1198927164077759, 'learning_rate': 2.5714285714285714e-05, 'epoch': 0.87}\n",
      "{'loss': 0.4809, 'grad_norm': 0.9895033836364746, 'learning_rate': 2.530612244897959e-05, 'epoch': 0.88}\n",
      "{'loss': 0.4531, 'grad_norm': 1.0243743658065796, 'learning_rate': 2.489795918367347e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5624, 'grad_norm': 1.3301185369491577, 'learning_rate': 2.448979591836735e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3905, 'grad_norm': 0.9993268251419067, 'learning_rate': 2.4081632653061224e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5131, 'grad_norm': 1.099151849746704, 'learning_rate': 2.3673469387755103e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3297, 'grad_norm': 0.8827643990516663, 'learning_rate': 2.326530612244898e-05, 'epoch': 0.89}\n",
      "{'loss': 0.4955, 'grad_norm': 1.0930016040802002, 'learning_rate': 2.2857142857142858e-05, 'epoch': 0.89}\n",
      "{'loss': 0.4128, 'grad_norm': 1.099252700805664, 'learning_rate': 2.2448979591836737e-05, 'epoch': 0.89}\n",
      "{'loss': 0.4356, 'grad_norm': 0.9465152621269226, 'learning_rate': 2.2040816326530613e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5267, 'grad_norm': 1.2217292785644531, 'learning_rate': 2.1632653061224492e-05, 'epoch': 0.89}\n",
      "{'loss': 0.4881, 'grad_norm': 1.151326298713684, 'learning_rate': 2.1224489795918368e-05, 'epoch': 0.9}\n",
      "{'loss': 0.4129, 'grad_norm': 1.0840100049972534, 'learning_rate': 2.0816326530612247e-05, 'epoch': 0.9}\n",
      "{'loss': 0.4164, 'grad_norm': 1.0226715803146362, 'learning_rate': 2.0408163265306123e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6194, 'grad_norm': 0.8511487245559692, 'learning_rate': 2e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6082, 'grad_norm': 0.9289188981056213, 'learning_rate': 1.9591836734693877e-05, 'epoch': 0.9}\n",
      "{'loss': 0.5359, 'grad_norm': 0.9021223783493042, 'learning_rate': 1.9183673469387756e-05, 'epoch': 0.91}\n",
      "{'loss': 0.5208, 'grad_norm': 0.9664037823677063, 'learning_rate': 1.8775510204081632e-05, 'epoch': 0.91}\n",
      "{'loss': 0.5344, 'grad_norm': 1.075745701789856, 'learning_rate': 1.836734693877551e-05, 'epoch': 0.91}\n",
      "{'loss': 0.5639, 'grad_norm': 1.0927135944366455, 'learning_rate': 1.7959183673469387e-05, 'epoch': 0.91}\n",
      "{'loss': 0.4837, 'grad_norm': 0.9603902697563171, 'learning_rate': 1.7551020408163266e-05, 'epoch': 0.91}\n",
      "{'loss': 0.495, 'grad_norm': 1.0070273876190186, 'learning_rate': 1.7142857142857145e-05, 'epoch': 0.92}\n",
      "{'loss': 0.4807, 'grad_norm': 1.0261679887771606, 'learning_rate': 1.673469387755102e-05, 'epoch': 0.92}\n",
      "{'loss': 0.4816, 'grad_norm': 0.9108940958976746, 'learning_rate': 1.6326530612244897e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5283, 'grad_norm': 1.0243663787841797, 'learning_rate': 1.5918367346938776e-05, 'epoch': 0.92}\n",
      "{'loss': 0.487, 'grad_norm': 0.9592723846435547, 'learning_rate': 1.5510204081632655e-05, 'epoch': 0.92}\n",
      "{'loss': 0.4364, 'grad_norm': 1.0025395154953003, 'learning_rate': 1.5102040816326532e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6462, 'grad_norm': 1.0413998365402222, 'learning_rate': 1.469387755102041e-05, 'epoch': 0.93}\n",
      "{'loss': 0.7083, 'grad_norm': 1.4046037197113037, 'learning_rate': 1.4285714285714285e-05, 'epoch': 0.93}\n",
      "{'loss': 0.4679, 'grad_norm': 1.0442979335784912, 'learning_rate': 1.3877551020408165e-05, 'epoch': 0.93}\n",
      "{'loss': 0.799, 'grad_norm': 1.1885578632354736, 'learning_rate': 1.3469387755102042e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5881, 'grad_norm': 1.1005101203918457, 'learning_rate': 1.306122448979592e-05, 'epoch': 0.94}\n",
      "{'loss': 0.577, 'grad_norm': 1.0784462690353394, 'learning_rate': 1.2653061224489795e-05, 'epoch': 0.94}\n",
      "{'loss': 0.4577, 'grad_norm': 0.9214097857475281, 'learning_rate': 1.2244897959183674e-05, 'epoch': 0.94}\n",
      "{'loss': 0.4626, 'grad_norm': 1.0228461027145386, 'learning_rate': 1.1836734693877552e-05, 'epoch': 0.94}\n",
      "{'loss': 0.5606, 'grad_norm': 1.1055210828781128, 'learning_rate': 1.1428571428571429e-05, 'epoch': 0.94}\n",
      "{'loss': 0.5339, 'grad_norm': 1.0721125602722168, 'learning_rate': 1.1020408163265306e-05, 'epoch': 0.95}\n",
      "{'loss': 0.4701, 'grad_norm': 0.9473869204521179, 'learning_rate': 1.0612244897959184e-05, 'epoch': 0.95}\n",
      "{'loss': 0.4435, 'grad_norm': 0.9476006031036377, 'learning_rate': 1.0204081632653061e-05, 'epoch': 0.95}\n",
      "{'loss': 0.5731, 'grad_norm': 1.273191213607788, 'learning_rate': 9.795918367346939e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5707, 'grad_norm': 1.282623529434204, 'learning_rate': 9.387755102040816e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5239, 'grad_norm': 1.0411789417266846, 'learning_rate': 8.979591836734694e-06, 'epoch': 0.96}\n",
      "{'loss': 0.5286, 'grad_norm': 1.0783106088638306, 'learning_rate': 8.571428571428573e-06, 'epoch': 0.96}\n",
      "{'loss': 0.3994, 'grad_norm': 0.949944794178009, 'learning_rate': 8.163265306122448e-06, 'epoch': 0.96}\n",
      "{'loss': 0.6187, 'grad_norm': 1.2086284160614014, 'learning_rate': 7.755102040816327e-06, 'epoch': 0.96}\n",
      "{'loss': 0.4429, 'grad_norm': 1.0567148923873901, 'learning_rate': 7.346938775510205e-06, 'epoch': 0.96}\n",
      "{'loss': 0.6016, 'grad_norm': 1.225046157836914, 'learning_rate': 6.938775510204082e-06, 'epoch': 0.97}\n",
      "{'loss': 0.4282, 'grad_norm': 1.0655752420425415, 'learning_rate': 6.53061224489796e-06, 'epoch': 0.97}\n",
      "{'loss': 0.5003, 'grad_norm': 1.098131537437439, 'learning_rate': 6.122448979591837e-06, 'epoch': 0.97}\n",
      "{'loss': 0.4295, 'grad_norm': 1.0490143299102783, 'learning_rate': 5.7142857142857145e-06, 'epoch': 0.97}\n",
      "{'loss': 0.4348, 'grad_norm': 1.0164947509765625, 'learning_rate': 5.306122448979592e-06, 'epoch': 0.97}\n",
      "{'loss': 0.523, 'grad_norm': 1.0777231454849243, 'learning_rate': 4.897959183673469e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6441, 'grad_norm': 1.4035418033599854, 'learning_rate': 4.489795918367347e-06, 'epoch': 0.98}\n",
      "{'loss': 0.4053, 'grad_norm': 1.052053689956665, 'learning_rate': 4.081632653061224e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6073, 'grad_norm': 1.2272619009017944, 'learning_rate': 3.6734693877551024e-06, 'epoch': 0.98}\n",
      "{'loss': 0.4057, 'grad_norm': 0.9755786657333374, 'learning_rate': 3.26530612244898e-06, 'epoch': 0.98}\n",
      "{'loss': 0.4621, 'grad_norm': 1.1100813150405884, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.99}\n",
      "{'loss': 0.3773, 'grad_norm': 1.0502243041992188, 'learning_rate': 2.4489795918367347e-06, 'epoch': 0.99}\n",
      "{'loss': 0.4433, 'grad_norm': 1.1045199632644653, 'learning_rate': 2.040816326530612e-06, 'epoch': 0.99}\n",
      "{'loss': 0.4037, 'grad_norm': 0.9979859590530396, 'learning_rate': 1.63265306122449e-06, 'epoch': 0.99}\n",
      "{'loss': 0.4263, 'grad_norm': 1.0685464143753052, 'learning_rate': 1.2244897959183673e-06, 'epoch': 0.99}\n",
      "{'loss': 0.3929, 'grad_norm': 1.271755576133728, 'learning_rate': 8.16326530612245e-07, 'epoch': 1.0}\n",
      "{'loss': 0.4416, 'grad_norm': 1.0908602476119995, 'learning_rate': 4.081632653061225e-07, 'epoch': 1.0}\n",
      "{'loss': 0.3249, 'grad_norm': 0.9775590896606445, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb88f2b8a97947719c8039b1fb9a7187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5205636024475098, 'eval_runtime': 358.3779, 'eval_samples_per_second': 0.558, 'eval_steps_per_second': 0.558, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\utils\\save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5559.2092, 'train_samples_per_second': 0.18, 'train_steps_per_second': 0.09, 'train_loss': 0.6572189555764198, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.6572189555764198, metrics={'train_runtime': 5559.2092, 'train_samples_per_second': 0.18, 'train_steps_per_second': 0.09, 'total_flos': 1382619445641216.0, 'train_loss': 0.6572189555764198, 'epoch': 1.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Fine Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting some test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = dataset.shuffle(seed=65).select(range(1200, 1205))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flags': 'BELMQZ',\n",
       " 'instruction': 'i do not know how to inform of a trouble withpayments',\n",
       " 'category': 'PAYMENT',\n",
       " 'intent': 'payment_issue',\n",
       " 'response': \"Don't worry, I'm here to guide you through the process of reporting any trouble you're experiencing with your payments. To inform us about the issue, you can reach out to our customer support team via phone or chat. They are available {{Customer Support Hours}} at {{Customer Support Phone Number}} or through the Live Chat on our website at {{Website URL}}. Our dedicated team will assist you step-by-step in notifying us about the trouble, ensuring that it gets resolved as quickly as possible. We appreciate you bringing this to our attention, and we want to assure you that we're committed to providing you with the best assistance for your payment concerns.\"}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting The Lora Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"C:/Users/User/Data Science/Deep Learning/Generative AI/Fine Tuning LLMs/fine-tuning llama 3.2 1B/llama-3.2-1b-it-Ecommerce-ChatBot/checkpoint-500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\utils\\modeling.py:856: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(base_model, return_dict=True, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128258"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128258, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128258, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128258, 2048)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128258, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128258, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128258, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the eval data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function that takes the instruction string and returns the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    If the question is out of context and not related to your job as a customer service agent, let the customer know that you can not help and they should look elsewhere for answers.\n",
    "    \"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": \"I have to see what payment payment modalities are accepted\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a top-rated customer service agent named John. \\n    Be polite to customers and answer all their questions.\\n    If the question is out of context and not related to your job as a customer service agent, let the customer know that you can not help and they should look elsewhere for answers.\\n    <|im_end|>\\n<|im_start|>user\\nI have to see what payment payment modalities are accepted<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_eval_template(row):\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    base_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model, trust_remote_code=True)\n",
    "\n",
    "    instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "        Be polite to customers and answer all their questions.\n",
    "        If the question is out of context and not related to your job as a customer service agent, let the customer know that you can not help and they should look elsewhere for answers.\n",
    "        \"\"\"\n",
    "\n",
    "    row_json = [{\"role\": \"system\", \"content\": instruction},\n",
    "                {\"role\": \"user\", \"content\": row[\"instruction\"]}]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m eval_prompt_1 \u001b[38;5;241m=\u001b[39m format_eval_template(\u001b[43meval_dataset\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eval_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "eval_prompt_1 = format_eval_template(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not know how to inform of a trouble withpayments'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset['instruction'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a top-rated customer service agent named John. \\n        Be polite to customers and answer all their questions.\\n        If the question is out of context and not related to your job as a customer service agent, let the customer know that you can not help and they should look elsewhere for answers.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ni do not know how to inform of a trouble withpayments<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prompt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eval_data_and_pred(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True).to(\"cuda\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    outputs = lora_model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flags': 'BELMQZ',\n",
       " 'instruction': 'i do not know how to inform of a trouble withpayments',\n",
       " 'category': 'PAYMENT',\n",
       " 'intent': 'payment_issue',\n",
       " 'response': \"Don't worry, I'm here to guide you through the process of reporting any trouble you're experiencing with your payments. To inform us about the issue, you can reach out to our customer support team via phone or chat. They are available {{Customer Support Hours}} at {{Customer Support Phone Number}} or through the Live Chat on our website at {{Website URL}}. Our dedicated team will assist you step-by-step in notifying us about the trouble, ensuring that it gets resolved as quickly as possible. We appreciate you bringing this to our attention, and we want to assure you that we're committed to providing you with the best assistance for your payment concerns.\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocess_eval_data_and_pred(eval_prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm here to help. Don't worry, I'm here to assist you. Informing us about a trouble with payments can be a bit tricky, but it's a common issue. Let me walk you through the steps.\n",
      "\n",
      "To start, could you please tell me the following information so I can better understand your concern?\n",
      "\n",
      "1. What payment method were you using (e.g., credit card, PayPal, etc.)?\n",
      "2. When did you first notice the issue?\n",
      "3. What happened after that (e.g., did you try to pay online or by phone, etc.)?\n",
      "4. Have you received any notifications or emails from us regarding the issue?\n",
      "\n",
      "Once I have this information, I can guide you through the process of resolving the issue\n"
     ]
    }
   ],
   "source": [
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128258, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128258, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "response = preprocess_eval_data_and_pred(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'll take care of it! I'm here to assist you with checking the payment modalities we accept. We offer a variety of options to ensure convenience and flexibility for our customers. You can choose from credit/debit cards, PayPal, bank transfer, Apple Pay, Google Pay, and Visa. If you have any specific questions or need further assistance with any of these options, please let me know. I'm here to help!\n"
     ]
    }
   ],
   "source": [
    "print(response.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_1 = [{\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": \"what is your name and what do you do\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = tokenizer.apply_chat_template(message_1, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt_2 = preprocess_eval_data_and_pred(prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello! My name is John, and I'm a customer service agent. I'm here to help you with any questions or concerns you may have. Please feel free to ask me anything, and I'll do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "print(eval_prompt_2.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_2 = [{\"role\": \"system\", \"content\": instruction},\n",
    "             {\"role\": \"user\", \"content\": \"what do you know about wind and air\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = tokenizer.apply_chat_template(\n",
    "    message_2, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = preprocess_eval_data_and_pred(prompt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hi there! As a customer service agent, I'm happy to help you with any questions or concerns you may have about wind and air. Wind and air are fascinating topics, and I'd be delighted to provide you with some information.\n",
      "\n",
      "Wind and air are essential components of our atmosphere, and understanding their behavior is crucial for various fields, including meteorology, aviation, and environmental science. Here are some key points about wind and air:\n",
      "\n",
      "1. **Wind direction and speed**: Wind direction and speed are measured in knots (kt) and miles per hour (mph). Wind direction is measured from 0 to 360 degrees, while wind speed is measured from 0 to 600 mph. Wind direction is typically measured from north to south, and wind\n"
     ]
    }
   ],
   "source": [
    "print(response_2.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the response above regarding a non customer support response, we can deduce that the datasets does not contain outliers and the response the model should give in that case wasnt specified even if we added such in the prompt `I can try a few shot inference prompt and see the effect on the response`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and Exporting Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_url = \"llama-3.2-1b-it-Ecommerce-customer_support\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128258, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\utils\\save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_model.save_pretrained(new_model_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama-3.2-1b-it-Ecommerce-customer_support\\\\tokenizer_config.json',\n",
       " 'llama-3.2-1b-it-Ecommerce-customer_support\\\\special_tokens_map.json',\n",
       " 'llama-3.2-1b-it-Ecommerce-customer_support\\\\tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(new_model_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_3 = [{\"role\": \"system\", \"content\": instruction},\n",
    "             {\"role\": \"user\", \"content\": \"i want to cancel my order, it is taking too long\"}]\n",
    "prompt_3 = tokenizer.apply_chat_template(\n",
    "    message_3, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_3 = preprocess_eval_data_and_pred(prompt_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I apologize for the inconvenience you're facing with your order. I understand that you would like to cancel it. I'm here to assist you in making the necessary changes. To proceed with the cancellation, could you please provide me with the order number? Once I have that information, I will guide you through the cancellation process step by step.\n"
     ]
    }
   ],
   "source": [
    "print(response_3.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_4 = [{\"role\": \"system\", \"content\": instruction},\n",
    "             {\"role\": \"user\", \"content\": \"i do not know how to inform of a trouble withpayments\"}]\n",
    "prompt_4 = tokenizer.apply_chat_template(\n",
    "    message_4, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_4 = preprocess_eval_data_and_pred(prompt_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I apologize for any inconvenience you may be facing with your payments. To inform us of the trouble you're experiencing, you can reach out to our dedicated customer support team through our website or by calling our toll-free number. They will be more than happy to assist you in resolving the issue and ensuring a smooth payment process for you. We appreciate your patience and cooperation.\n"
     ]
    }
   ],
   "source": [
    "print(response_4.split(\"assistant\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
